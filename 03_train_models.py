# 03_train_models.py
# Script d'entra√Ænement et comparaison de mod√®les

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import pickle
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print(" " * 15 + "ENTRA√éNEMENT DES MOD√àLES")
print("=" * 70)

# ============================================
# 1. CHARGEMENT DES DONN√âES PR√âTRAIT√âES
# ============================================
print("\n[1] CHARGEMENT DES DONN√âES PR√âTRAIT√âES")
print("-" * 70)

X_train = pd.read_csv('dataset/processed/X_train.csv')
X_val = pd.read_csv('dataset/processed/X_val.csv')
y_train = pd.read_csv('dataset/processed/y_train.csv').values.ravel()
y_val = pd.read_csv('dataset/processed/y_val.csv').values.ravel()

print(f"‚úì X_train : {X_train.shape}")
print(f"‚úì X_val   : {X_val.shape}")
print(f"‚úì y_train : {y_train.shape}")
print(f"‚úì y_val   : {y_val.shape}")

# ============================================
# 2. FONCTION D'√âVALUATION
# ============================================
def evaluate_model(y_true, y_pred, model_name):
    """√âvalue un mod√®le avec plusieurs m√©triques"""
    # Retransformation inverse du log
    y_true_original = np.expm1(y_true)
    y_pred_original = np.expm1(y_pred)
    
    # M√©triques
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    rmse_original = np.sqrt(mean_squared_error(y_true_original, y_pred_original))
    mae = mean_absolute_error(y_true_original, y_pred_original)
    r2 = r2_score(y_true, y_pred)
    mape = np.mean(np.abs((y_true_original - y_pred_original) / y_true_original)) * 100
    
    print(f"\n{'='*70}")
    print(f"  R√âSULTATS : {model_name}")
    print(f"{'='*70}")
    print(f"  RMSE (log)                : {rmse:.4f}")
    print(f"  RMSE (prix r√©el)          : ${rmse_original:,.2f}")
    print(f"  MAE (prix r√©el)           : ${mae:,.2f}")
    print(f"  R¬≤ Score                  : {r2:.4f} ({r2*100:.2f}%)")
    print(f"  MAPE (erreur %)           : {mape:.2f}%")
    print(f"{'='*70}")
    
    return {
        'model': model_name,
        'rmse': rmse,
        'rmse_original': rmse_original,
        'mae': mae,
        'r2': r2,
        'mape': mape
    }

# ============================================
# 3. MOD√àLE 1 : R√âGRESSION LIN√âAIRE
# ============================================
print("\n[2] ENTRA√éNEMENT : R√âGRESSION LIN√âAIRE")
print("-" * 70)

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_val)

results_lr = evaluate_model(y_val, y_pred_lr, "R√©gression Lin√©aire")

# ============================================
# 4. MOD√àLE 2 : RIDGE REGRESSION
# ============================================
print("\n[3] ENTRA√éNEMENT : RIDGE REGRESSION (avec r√©gularisation)")
print("-" * 70)

ridge_model = Ridge(alpha=10.0, random_state=42)
ridge_model.fit(X_train, y_train)
y_pred_ridge = ridge_model.predict(X_val)

results_ridge = evaluate_model(y_val, y_pred_ridge, "Ridge Regression")

# ============================================
# 5. MOD√àLE 3 : LASSO REGRESSION
# ============================================
print("\n[4] ENTRA√éNEMENT : LASSO REGRESSION (s√©lection de features)")
print("-" * 70)

lasso_model = Lasso(alpha=0.0005, random_state=42, max_iter=10000)
lasso_model.fit(X_train, y_train)
y_pred_lasso = lasso_model.predict(X_val)

results_lasso = evaluate_model(y_val, y_pred_lasso, "Lasso Regression")

# ============================================
# 6. MOD√àLE 4 : RANDOM FOREST
# ============================================
print("\n[5] ENTRA√éNEMENT : RANDOM FOREST")
print("-" * 70)
print("‚è≥ Entra√Ænement en cours (cela peut prendre 1-2 minutes)...")

rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_val)

results_rf = evaluate_model(y_val, y_pred_rf, "Random Forest")

# ============================================
# 7. MOD√àLE 5 : XGBOOST (LE MEILLEUR!)
# ============================================
print("\n[6] ENTRA√éNEMENT : XGBOOST")
print("-" * 70)
print("‚è≥ Entra√Ænement en cours (cela peut prendre 1-2 minutes)...")

xgb_model = xgb.XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=4,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbosity=0
)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_val)

results_xgb = evaluate_model(y_val, y_pred_xgb, "XGBoost")

# ============================================
# 8. COMPARAISON DES MOD√àLES
# ============================================
print("\n[7] COMPARAISON DES MOD√àLES")
print("-" * 70)

# Tableau comparatif
results_df = pd.DataFrame([results_lr, results_ridge, results_lasso, results_rf, results_xgb])
results_df = results_df.sort_values('rmse')

print("\nüìä TABLEAU COMPARATIF (tri√© par RMSE)")
print("="*70)
print(results_df.to_string(index=False))

# Meilleur mod√®le
best_model_name = results_df.iloc[0]['model']
best_rmse = results_df.iloc[0]['rmse']
best_r2 = results_df.iloc[0]['r2']

print(f"\n{'='*70}")
print(f"üèÜ MEILLEUR MOD√àLE : {best_model_name}")
print(f"   RMSE : {best_rmse:.4f}")
print(f"   R¬≤   : {best_r2:.4f} ({best_r2*100:.2f}%)")
print(f"{'='*70}")

# ============================================
# 9. VISUALISATIONS
# ============================================
print("\n[8] G√âN√âRATION DES VISUALISATIONS")
print("-" * 70)

# Graphique 1 : Comparaison des RMSE
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('COMPARAISON DES MOD√àLES', fontsize=16, fontweight='bold')

# RMSE Comparison
axes[0, 0].barh(results_df['model'], results_df['rmse'], color='steelblue')
axes[0, 0].set_xlabel('RMSE (log scale)')
axes[0, 0].set_title('RMSE - Plus bas = meilleur')
axes[0, 0].invert_yaxis()

# R¬≤ Score Comparison
axes[0, 1].barh(results_df['model'], results_df['r2'], color='green')
axes[0, 1].set_xlabel('R¬≤ Score')
axes[0, 1].set_title('R¬≤ Score - Plus haut = meilleur')
axes[0, 1].invert_yaxis()

# MAE Comparison
axes[1, 0].barh(results_df['model'], results_df['mae'], color='coral')
axes[1, 0].set_xlabel('MAE ($)')
axes[1, 0].set_title('Mean Absolute Error - Plus bas = meilleur')
axes[1, 0].invert_yaxis()

# MAPE Comparison
axes[1, 1].barh(results_df['model'], results_df['mape'], color='purple')
axes[1, 1].set_xlabel('MAPE (%)')
axes[1, 1].set_title('Mean Absolute Percentage Error - Plus bas = meilleur')
axes[1, 1].invert_yaxis()

plt.tight_layout()
plt.savefig('dataset/processed/model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úì Graphique de comparaison sauvegard√© : model_comparison.png")

# Graphique 2 : Pr√©dictions vs Valeurs r√©elles (XGBoost)
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle('QUALIT√â DES PR√âDICTIONS - XGBOOST', fontsize=16, fontweight='bold')

# Prix en √©chelle log
axes[0].scatter(y_val, y_pred_xgb, alpha=0.5, s=30)
axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 
             'r--', lw=2, label='Pr√©diction parfaite')
axes[0].set_xlabel('Prix r√©el (log)')
axes[0].set_ylabel('Prix pr√©dit (log)')
axes[0].set_title('Pr√©dictions vs R√©alit√© (√©chelle log)')
axes[0].legend()

# Prix en dollars
y_val_original = np.expm1(y_val)
y_pred_xgb_original = np.expm1(y_pred_xgb)
axes[1].scatter(y_val_original, y_pred_xgb_original, alpha=0.5, s=30, color='green')
axes[1].plot([y_val_original.min(), y_val_original.max()], 
             [y_val_original.min(), y_val_original.max()], 
             'r--', lw=2, label='Pr√©diction parfaite')
axes[1].set_xlabel('Prix r√©el ($)')
axes[1].set_ylabel('Prix pr√©dit ($)')
axes[1].set_title('Pr√©dictions vs R√©alit√© (dollars)')
axes[1].legend()

plt.tight_layout()
plt.savefig('dataset/processed/predictions_xgboost.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úì Graphique des pr√©dictions sauvegard√© : predictions_xgboost.png")

# Graphique 3 : Feature Importance (XGBoost)
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False).head(20)

plt.figure(figsize=(12, 8))
plt.barh(range(len(feature_importance)), feature_importance['importance'], color='teal')
plt.yticks(range(len(feature_importance)), feature_importance['feature'])
plt.xlabel('Importance')
plt.title('TOP 20 DES FEATURES LES PLUS IMPORTANTES (XGBoost)', 
          fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('dataset/processed/feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úì Graphique d'importance des features sauvegard√© : feature_importance.png")

# ============================================
# 10. SAUVEGARDE DES MOD√àLES
# ============================================
print("\n[9] SAUVEGARDE DES MOD√àLES")
print("-" * 70)

# Cr√©er dossier models
import os
if not os.path.exists('models'):
    os.makedirs('models')

# Sauvegarde de tous les mod√®les
with open('models/linear_regression.pkl', 'wb') as f:
    pickle.dump(lr_model, f)
print("‚úì linear_regression.pkl")

with open('models/ridge_regression.pkl', 'wb') as f:
    pickle.dump(ridge_model, f)
print("‚úì ridge_regression.pkl")

with open('models/lasso_regression.pkl', 'wb') as f:
    pickle.dump(lasso_model, f)
print("‚úì lasso_regression.pkl")

with open('models/random_forest.pkl', 'wb') as f:
    pickle.dump(rf_model, f)
print("‚úì random_forest.pkl")

with open('models/xgboost.pkl', 'wb') as f:
    pickle.dump(xgb_model, f)
print("‚úì xgboost.pkl (MEILLEUR MOD√àLE)")

# Sauvegarde des r√©sultats
results_df.to_csv('models/model_comparison.csv', index=False)
print("‚úì model_comparison.csv")

print("\n" + "=" * 70)
print(" " * 15 + "‚úì ENTRA√éNEMENT TERMIN√â !")
print("=" * 70)
print(f"\nüèÜ Meilleur mod√®le : {best_model_name}")
print(f"üìä Pr√©cision : {best_r2*100:.2f}%")
print(f"üí∞ Erreur moyenne : ${results_df.iloc[0]['mae']:,.2f}")
print("\nProchaine √©tape : Pr√©dictions sur le test set (04_predict.py)")